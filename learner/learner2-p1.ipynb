{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T05:40:07.161060Z",
     "start_time": "2022-04-04T05:40:05.656236Z"
    },
    "id": "WYq1Yz06breD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTc5ERo3FGRN"
   },
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T05:40:13.620965Z",
     "start_time": "2022-04-04T05:40:09.797930Z"
    },
    "id": "puKPHtnH0pIG"
   },
   "outputs": [],
   "source": [
    "s1 = pd.read_csv(\"../s1.csv\")\n",
    "df2001 = pd.read_csv(\"../df2001.csv\")\n",
    "df2003 = pd.read_csv(\"../df2003.csv\")\n",
    "df2005 = pd.read_csv(\"../df2005.csv\")\n",
    "df2007 = pd.read_csv(\"../df2007.csv\")\n",
    "df2009 = pd.read_csv(\"../df2009.csv\")\n",
    "df2011 = pd.read_csv(\"../df2011.csv\")\n",
    "df2013 = pd.read_csv(\"../df2013.csv\")\n",
    "df2015 = pd.read_csv(\"../df2015.csv\")\n",
    "df2017 = pd.read_csv(\"../df2017.csv\")\n",
    "df2019 = pd.read_csv(\"../df2019.csv\")\n",
    "df2021 = pd.read_csv(\"../df2021.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZd_8wpNFIYx"
   },
   "source": [
    "## Droping cols and OneHotEncoding aka set dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T05:41:10.286305Z",
     "start_time": "2022-04-04T05:41:10.272298Z"
    },
    "id": "3jO-8xydbpbp"
   },
   "outputs": [],
   "source": [
    "def preprocess(csv_file):\n",
    "  csv_file = csv_file.drop(['lease_commence_date'], axis = 1)\n",
    "  csv_file = csv_file.drop(['Unnamed: 0'],axis = 1)\n",
    "  csv_file = csv_file.drop(['address'],axis = 1)\n",
    "\n",
    "  onehot = OneHotEncoder()\n",
    "\n",
    "  town_df = pd.DataFrame(onehot.fit_transform(csv_file[['town']]).toarray())\n",
    "  town_df.columns = onehot.get_feature_names(['town'])\n",
    "\n",
    "  flat_model_df = pd.DataFrame(onehot.fit_transform(csv_file[['flat_model']]).toarray())\n",
    "  flat_model_df.columns = onehot.get_feature_names(['flat_model'])\n",
    "\n",
    "  flat_type_df = pd.DataFrame(onehot.fit_transform(csv_file[['flat_type']]).toarray())\n",
    "  flat_type_df.columns = onehot.get_feature_names(['flat_type'])\n",
    "\n",
    "  csv_file = csv_file.join([town_df, flat_model_df, flat_type_df])\n",
    "  csv_file = csv_file.drop(['town','flat_type','flat_model'], axis = 1)\n",
    "\n",
    "  return csv_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-04T05:41:11.812588Z",
     "start_time": "2022-04-04T05:41:11.783524Z"
    },
    "id": "mfVnM0fIOIUf"
   },
   "outputs": [],
   "source": [
    "s1     = preprocess(s1)\n",
    "df2001 = preprocess(df2001)\n",
    "df2003 = preprocess(df2003)\n",
    "df2005 = preprocess(df2005)\n",
    "df2007 = preprocess(df2007)\n",
    "df2009 = preprocess(df2009)\n",
    "df2011 = preprocess(df2011)\n",
    "df2013 = preprocess(df2013)\n",
    "df2015 = preprocess(df2015)\n",
    "df2017 = preprocess(df2017)\n",
    "df2019 = preprocess(df2019)\n",
    "df2021 = preprocess(df2021)\n",
    "\n",
    "savefile = open(\"savefile.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejj4tNAdFZ6y"
   },
   "source": [
    "## Spliting data to training and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set S1.1\n",
    "s1 = s1.loc[s1['k_fold'] == 1]\n",
    "s1 = s1.drop(['k_fold'],axis = 1)\n",
    "\n",
    "# \"Classes\" / actual values\n",
    "y_test = s1['resale_price'].tolist()\n",
    "\n",
    "# Values to test on\n",
    "s1  = s1.drop(['resale_price'], axis=1)\n",
    "x_test = s1.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "f20M4R0bVUsj"
   },
   "outputs": [],
   "source": [
    "def k_fold(dataset, k: int, k1: int = 1, SSE_valid_acc : float = 0, SSE_test_acc: float = 0) -> list:\n",
    "  for fold_num in range(k1, k + 1):\n",
    "    print(f'Fold number {fold_num}...')\n",
    "    # Validation set\n",
    "    validation    = dataset.loc[dataset['k_fold'] == fold_num]\n",
    "    validation    = validation.drop(['k_fold'], axis = 1)\n",
    "    y_validation  = validation['resale_price'].tolist()\n",
    "    validation    = validation.drop(['resale_price'], axis = 1)\n",
    "    x_validation  = validation.to_numpy()\n",
    "\n",
    "    # Training set\n",
    "    train   = dataset.loc[dataset['k_fold'] != fold_num]\n",
    "    train   = train.drop(['k_fold'], axis = 1)\n",
    "    y_train = train['resale_price'].tolist()\n",
    "    train   = train.drop(['resale_price'], axis = 1)\n",
    "    x_train = train.to_numpy()\n",
    "\n",
    "    # =========== modeling ==========\n",
    "    \n",
    "    # Support Vector Regression\n",
    "    model = svm.SVR()\n",
    "\n",
    "    # Random Forest\n",
    "    # model = RandomForestRegressor(n_estimators = 110, max_depth = 14, random_state = 0)\n",
    "    model = model.fit(x_train, y_train)\n",
    "    y_validation_pred = model.predict(x_validation)\n",
    "\n",
    "    SSE_valid = sum((y_validation - y_validation_pred)**2)\n",
    "    SSE_valid_acc += SSE_valid\n",
    "\n",
    "    y_test_pred = model.predict(x_test)\n",
    "\n",
    "    SSE_test = sum((y_test - y_test_pred)**2)\n",
    "    SSE_test_acc += SSE_test\n",
    "    savefile.write(f'Fold number {fold_num}\\n')\n",
    "    savefile.write(f'SSE_valid = {SSE_valid}\\n')\n",
    "    savefile.write(f'SSE_test = {SSE_test}\\n')\n",
    "    savefile.flush()\n",
    "    \n",
    "  SSE_valid_avg = SSE_valid_acc / k\n",
    "  SSE_test_avg = SSE_test_acc / k\n",
    "\n",
    "  savefile.write(f'K-fold Cross validation. Selected K = {k}\\n')\n",
    "  savefile.write(f'Average validation SSE = {SSE_valid_avg}\\n')\n",
    "  savefile.write(f'Average testing SSE = {SSE_test_avg}\\n')\n",
    "  savefile.flush()\n",
    "\n",
    "  print(f'K-fold Cross Validation. Selected K = {k}')\n",
    "  print(f'Average validation Sum of Squared Errors = {SSE_valid_avg}')\n",
    "  print(f'Average testing Sum of Squared Errors = {SSE_test_avg}')\n",
    "\n",
    "  return [SSE_valid_avg, SSE_test_avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 943
    },
    "id": "18NueEEhWpjm",
    "outputId": "86078b55-8881-48dc-94d9-2207563bcca7"
   },
   "outputs": [],
   "source": [
    "avg_results = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "year_range = {0 : \"2001\",\n",
    "              1 : \"2003\",\n",
    "              2 : \"2005\",\n",
    "              3 : \"2007\",\n",
    "              4 : \"2009\",\n",
    "              5 : \"2011\",\n",
    "              6 : \"2013\",\n",
    "              7 : \"2015\",\n",
    "              8 : \"2017\",\n",
    "              9 : \"2019\",\n",
    "              10: \"2021\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number 3...\n",
      "Fold number 4...\n"
     ]
    }
   ],
   "source": [
    "avg_results[5]= k_fold(df2011, 10, k1=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results[6]= k_fold(df2013, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results[7]= k_fold(df2015, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number 1...\n",
      "Fold number 2...\n",
      "Fold number 3...\n",
      "Fold number 4...\n"
     ]
    }
   ],
   "source": [
    "avg_results[8]= k_fold(df2017, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_results[9]= k_fold(df2019, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold number 1...\n",
      "Fold number 2...\n",
      "Fold number 3...\n",
      "Fold number 4...\n",
      "Fold number 5...\n",
      "Fold number 6...\n",
      "Fold number 7...\n",
      "Fold number 8...\n",
      "Fold number 9...\n",
      "Fold number 10...\n",
      "K-fold Cross Validation. Selected K = 10\n",
      "Average validation Sum of Squared Errors = 895475578354.0979\n",
      "Average testing Sum of Squared Errors = 4927769460126.204\n"
     ]
    }
   ],
   "source": [
    "add_flat = np.zeros(11553)\n",
    "df2021[\"flat_model_Premium Maisonette\"] = add_flat\n",
    "avg_results[10]= k_fold(df2021, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding smallest error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best starting year for validation: 2021\n",
      "Best startingyear for testing: 2017\n"
     ]
    }
   ],
   "source": [
    "# Lowest error\n",
    "# [validation, test]\n",
    "min_error = [-1, -1]\n",
    "\n",
    "# Which range gives the lowest error\n",
    "# [validation, test]\n",
    "min_error_index = [-1, -1]\n",
    "\n",
    "\n",
    "for col in range(2):\n",
    "    for result in range(11):\n",
    "        if (min_error[col] == -1):\n",
    "            min_error[col] = avg_results[result][col]\n",
    "            min_error_index[col] = result\n",
    "            continue\n",
    "        curr_error = min_error[col]\n",
    "        seen_error = avg_results[result][col]\n",
    "        if (curr_error > seen_error): # if smaller\n",
    "            min_error[col] = seen_error\n",
    "            min_error_index[col] = result\n",
    "\n",
    "print(f'Best starting year for validation: {year_range[min_error_index[0]]}')\n",
    "print(f'Best starting year for testing: {year_range[min_error_index[1]]}')\n",
    "\n",
    "savefile.write(\"Finished!\\n\")\n",
    "savefile.write(f'Best starting year for validation: {year_range[min_error_index[0]]}\\n')\n",
    "savefile.write(f'Best starting year for testing: {year_range[min_error_index[1]]}\\n')\n",
    "savefile.flush()\n",
    "savefile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(avg_results)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of hdb.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
